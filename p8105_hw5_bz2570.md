p8105_hw5_bz2570
================
Boran Zhai
2025-11-06

## Problem 1

``` r
birthday_sim <- function(group_size) {
  birthdays <- sample(1:365, group_size, replace = TRUE)
  repeated_bday <- length(unique(birthdays)) < group_size
  repeated_bday
}

bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    result = map_lgl(bdays, birthday_sim)
  ) |>
  group_by(
    bdays
  ) |>
  summarize(
    prob_repeat = mean(result)
  )
knitr::kable(head(bday_sim_results, 10), caption = "Birthday simulation results (Only showing first 10 rows)", digits = 4)
```

| bdays | prob_repeat |
|------:|------------:|
|     2 |      0.0026 |
|     3 |      0.0093 |
|     4 |      0.0159 |
|     5 |      0.0304 |
|     6 |      0.0415 |
|     7 |      0.0597 |
|     8 |      0.0805 |
|     9 |      0.0947 |
|    10 |      0.1150 |
|    11 |      0.1364 |

Birthday simulation results (Only showing first 10 rows)

``` r
bday_sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15))
```

![](p8105_hw5_bz2570_files/figure-gfm/unnamed-chunk-2-1.png)<!-- -->

## Problem 2

``` r
# Set design elements:
n <- 30
sigma <- 5
alpha <- 0.05
# Set μ = 0. Generate 5000 datasets
mu_true_values <- c(0)
n_sims <- 5000
```

``` r
# Write a function
power_sim <- function(true_mu, n, sigma, n_sims) {
  results <- 
    expand_grid(
      mu = true_mu,           # The true value of mu
      iter = 1:n_sims
    ) |> 
    mutate(
      x = map(mu, ~rnorm(n, mean = .x, sd = sigma)),
      t_test = map(x, ~t.test(.x, mu = 0)),
      tidy_result = map(t_test, broom::tidy)
    ) |> 
    unnest(tidy_result) |> 
    
    select(mu, mu_hat = estimate, p_value = p.value)
  return(results)
}

sim_results_0 <- power_sim(mu_true_values, n, sigma, n_sims)
```

``` r
mu_true_values_add <- c(1, 2, 3, 4, 5, 6)
sim_results_add <-  power_sim(mu_true_values_add, n, sigma, n_sims)
# Combine results
sim_results <- bind_rows(sim_results_0, sim_results_add)
```

``` r
summary_stats <- 
  sim_results |> 
  group_by(mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat),           # Average estimate from all samples
    power = mean(p_value < alpha),        # The proportion of times the null was rejected
    avg_mu_hat_rejected = mean(mu_hat[p_value < alpha]),   
    # Average estimate of mu_hat only in samples for which the null was rejecte
    n_rejected = sum(p_value < alpha)     # Number of samples for which the null was rejected
  )
knitr::kable(summary_stats)
```

|  mu | avg_mu_hat |  power | avg_mu_hat_rejected | n_rejected |
|----:|-----------:|-------:|--------------------:|-----------:|
|   0 | -0.0070009 | 0.0514 |          -0.1015931 |        257 |
|   1 |  0.9964707 | 0.1898 |           2.2511888 |        949 |
|   2 |  1.9763759 | 0.5472 |           2.6114504 |       2736 |
|   3 |  2.9934642 | 0.8810 |           3.1912463 |       4405 |
|   4 |  4.0068404 | 0.9868 |           4.0370034 |       4934 |
|   5 |  5.0116598 | 0.9996 |           5.0129285 |       4998 |
|   6 |  6.0141538 | 1.0000 |           6.0141538 |       5000 |

``` r
power_plot <- 
  ggplot(summary_stats, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Association between Effect Size and Power",
    subtitle = "One-sample t-test with n = 30, σ = 5, α = 0.05",
    x = "True Value of μ",
    y = "Power"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
        plot.subtitle = element_text(hjust = 0.5))

print(power_plot)
```

![](p8105_hw5_bz2570_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

##### Describe the association between effect size and power

``` r
# Make two plot showing the average estimate of μ and the true value of μ / the average estimate of μ only in samples for which the null was rejected and the true value of μ
comparison_plot <- 
  ggplot(summary_stats) +
  geom_line(aes(x = mu, y = avg_mu_hat, color = "Average estimate of μ in all samples")) +
  geom_point(aes(x = mu, y = avg_mu_hat, color = "Average estimate of μ in all samples")) +
  geom_line(aes(x = mu, y = avg_mu_hat_rejected, color = "Average estimate of μ in samples where null was rejected")) +
  geom_point(aes(x = mu, y = avg_mu_hat_rejected, color = "Average estimate of μ in samples where null was rejected")) +
  labs(
    title = "Comparison of average estimates μ of and true μ in different samples",
    subtitle = "All samples vs samples where null was rejected",
    x = "True Value of μ",
    y = "Average Estimate of mu_hat",
    color = "Sample Type"
  ) +
  theme_minimal() +
  scale_color_manual(
    values = c("Average estimate of μ in all samples" = "red", 
               "Average estimate of μ in samples where null was rejected" = "blue")
  ) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom")
  
print(comparison_plot)
```

![](p8105_hw5_bz2570_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

##### Is the sample average of mu_hat across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

## Problem 3

``` r
homicide_data = read_csv("data/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Create city_state variable and summarize within cities to obtain the total number of homicides and the number of unsolved homicides
homicide_summary <- 
  homicide_data |> 
  janitor::clean_names() |> 
  mutate(
    city_state = str_c(city, ", ", state) 
  ) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),  # Total number of homicides
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))  # Number of unsolved homicides
  )

knitr::kable(homicide_summary)
```

| city_state         | total_homicides | unsolved_homicides |
|:-------------------|----------------:|-------------------:|
| Albuquerque, NM    |             378 |                146 |
| Atlanta, GA        |             973 |                373 |
| Baltimore, MD      |            2827 |               1825 |
| Baton Rouge, LA    |             424 |                196 |
| Birmingham, AL     |             800 |                347 |
| Boston, MA         |             614 |                310 |
| Buffalo, NY        |             521 |                319 |
| Charlotte, NC      |             687 |                206 |
| Chicago, IL        |            5535 |               4073 |
| Cincinnati, OH     |             694 |                309 |
| Columbus, OH       |            1084 |                575 |
| Dallas, TX         |            1567 |                754 |
| Denver, CO         |             312 |                169 |
| Detroit, MI        |            2519 |               1482 |
| Durham, NC         |             276 |                101 |
| Fort Worth, TX     |             549 |                255 |
| Fresno, CA         |             487 |                169 |
| Houston, TX        |            2942 |               1493 |
| Indianapolis, IN   |            1322 |                594 |
| Jacksonville, FL   |            1168 |                597 |
| Kansas City, MO    |            1190 |                486 |
| Las Vegas, NV      |            1381 |                572 |
| Long Beach, CA     |             378 |                156 |
| Los Angeles, CA    |            2257 |               1106 |
| Louisville, KY     |             576 |                261 |
| Memphis, TN        |            1514 |                483 |
| Miami, FL          |             744 |                450 |
| Milwaukee, wI      |            1115 |                403 |
| Minneapolis, MN    |             366 |                187 |
| Nashville, TN      |             767 |                278 |
| New Orleans, LA    |            1434 |                930 |
| New York, NY       |             627 |                243 |
| Oakland, CA        |             947 |                508 |
| Oklahoma City, OK  |             672 |                326 |
| Omaha, NE          |             409 |                169 |
| Philadelphia, PA   |            3037 |               1360 |
| Phoenix, AZ        |             914 |                504 |
| Pittsburgh, PA     |             631 |                337 |
| Richmond, VA       |             429 |                113 |
| Sacramento, CA     |             376 |                139 |
| San Antonio, TX    |             833 |                357 |
| San Bernardino, CA |             275 |                170 |
| San Diego, CA      |             461 |                175 |
| San Francisco, CA  |             663 |                336 |
| Savannah, GA       |             246 |                115 |
| St. Louis, MO      |            1677 |                905 |
| Stockton, CA       |             444 |                266 |
| Tampa, FL          |             208 |                 95 |
| Tulsa, AL          |               1 |                  0 |
| Tulsa, OK          |             583 |                193 |
| Washington, DC     |            1345 |                589 |
