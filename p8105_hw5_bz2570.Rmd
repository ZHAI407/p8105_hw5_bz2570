---
title: "p8105_hw5_bz2570"
author: "Boran Zhai"
date: "2025-11-06"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(readr)

set.seed(321)
```

## Problem 1

```{r}
birthday_sim <- function(group_size) {
  birthdays <- sample(1:365, group_size, replace = TRUE)
  repeated_bday <- length(unique(birthdays)) < group_size
  repeated_bday
}

bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) |>
  mutate(
    result = map_lgl(bdays, birthday_sim)
  ) |>
  group_by(
    bdays
  ) |>
  summarize(
    prob_repeat = mean(result)
  )
knitr::kable(head(bday_sim_results, 10), caption = "Birthday simulation results (Only showing first 10 rows)", digits = 4)
```

```{r}
bday_sim_results |>
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability of Shared Birthday by Group Size", 
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15))
```

#### Comment on results:

The probability of shared birthday grows obviously with group size, especially between group size of 10 and 35. For a group size of `r bday_sim_results$bdays[which(bday_sim_results$prob_repeat >= 0.5)[1]]`, the probability of at least two people sharing a birthday exceeds 50%. By group size `r bday_sim_results$bdays[which(bday_sim_results$prob_repeat >= 0.8)[1]]`, the probability exceeds 90%, and reaches near 1.00 (`r round(max(bday_sim_results$prob_repeat), 2)`) by group size 50.

## Problem 2

```{r}
# Set design elements:
n <- 30
sigma <- 5
alpha <- 0.05
# Set μ = 0. Generate 5000 datasets
mu_true_values <- c(0)
n_sims <- 5000
```

```{r}
# Write a function
power_sim <- function(true_mu, n, sigma, n_sims) {
  results <- 
    expand_grid(
      mu = true_mu,           # The true value of mu
      iter = 1:n_sims
    ) |> 
    mutate(
      x = map(mu, ~rnorm(n, mean = .x, sd = sigma)),
      t_test = map(x, ~t.test(.x, mu = 0)),
      tidy_result = map(t_test, broom::tidy)
    ) |> 
    unnest(tidy_result) |> 
    
    select(mu, mu_hat = estimate, p_value = p.value)
  return(results)
}

sim_results_0 <- power_sim(mu_true_values, n, sigma, n_sims)
```

```{r}
mu_true_values_add <- c(1, 2, 3, 4, 5, 6)
sim_results_add <-  power_sim(mu_true_values_add, n, sigma, n_sims)
# Combine results
sim_results <- bind_rows(sim_results_0, sim_results_add)
```

```{r}
summary_stats <- 
  sim_results |> 
  group_by(mu) |> 
  summarize(
    avg_mu_hat = mean(mu_hat),           # Average estimate from all samples
    power = mean(p_value < alpha),        # The proportion of times the null was rejected
    avg_mu_hat_rejected = mean(mu_hat[p_value < alpha]),   
    # Average estimate of mu_hat only in samples for which the null was rejecte
    n_rejected = sum(p_value < alpha)     # Number of samples for which the null was rejected
  )
knitr::kable(summary_stats)
```

```{r}
power_plot <- 
  ggplot(summary_stats, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Association between Effect Size and Power",
    subtitle = "One-sample t-test with n = 30, σ = 5, α = 0.05",
    x = "True Value of μ",
    y = "Power"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
        plot.subtitle = element_text(hjust = 0.5))

print(power_plot)
```

##### Describe the association between effect size and power

In this one-sample t-test, there is a strong positive association between effect size (true μ) and statistical power. The plot shows a sharp increase in power as μ rises from 1 to 4: starting at `r round(summary_stats$power[2], 3)` when μ = 1, increasing to `r round(summary_stats$power[3], 3)` at μ = 2, `r round(summary_stats$power[4], 3)` at μ = 3, and reaching `r round(summary_stats$power[5], 3)` at μ = 4. Beyond μ = 4, the curve flattens significantly, with power approaching `r round(summary_stats$power[7], 3)` by μ = 6.

```{r}
# Make two plot showing the average estimate of μ and the true value of μ / the average estimate of μ only in samples for which the null was rejected and the true value of μ
comparison_plot <- 
  ggplot(summary_stats) +
  geom_line(aes(x = mu, y = avg_mu_hat, color = "Average estimate of μ in all samples")) +
  geom_point(aes(x = mu, y = avg_mu_hat, color = "Average estimate of μ in all samples")) +
  geom_line(aes(x = mu, y = avg_mu_hat_rejected, color = "Average estimate of μ in samples where null was rejected")) +
  geom_point(aes(x = mu, y = avg_mu_hat_rejected, color = "Average estimate of μ in samples where null was rejected")) +
  labs(
    title = "Comparison of average estimates μ of and true μ in different samples",
    subtitle = "All samples vs samples where null was rejected",
    x = "True Value of μ",
    y = "Average Estimate of mu_hat",
    color = "Sample Type"
  ) +
  theme_minimal() +
  scale_color_manual(
    values = c("Average estimate of μ in all samples" = "red", 
               "Average estimate of μ in samples where null was rejected" = "blue")
  ) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 15),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "bottom") +
  guides(color = guide_legend(nrow = 2)) 
  
print(comparison_plot)
```

##### Is the sample average of mu_hat across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

No, the sample average of mu_hat across tests for which the null is rejected is not completely equal to the true value of μ, particularly for smaller effect sizes. This discrepancy arises from selection bias. When we condition on rejecting the null hypothesis, we select samples where mu_hat is extreme enough to be statistically significant. For μ = 1, the average mu_hat in rejected samples is `r round(summary_stats$avg_mu_hat_rejected[2], 3)` compared to the true value of 1 (bias = `r round(summary_stats$avg_mu_hat_rejected[2] - 1, 3)`). As μ increases to 6, the bias decreases to `r round(summary_stats$avg_mu_hat_rejected[7] - 6, 3)`. This occurs because with small effects, only the unusually large estimates are significant enough to reject the null, while with large effects, almost every sample shows a significant result.

## Problem 3 
```{r}
homicide_data = read_csv("data/homicide-data.csv")
```

#### Describe the raw data.

The raw dataset contains `r nrow(homicide_data)` homicide records and `r ncol(homicide_data)` variables. Key variables include victim demographics like *name, age, race, sex*, geographic information like *city, state, lat, lon*, and *disposition* (which indicates whether cases were closed by arrest, closed without arrest, or remain open).

#### Create city_state variable and summarize
```{r}
city_homicides <- 
  homicide_data |>
  janitor::clean_names() |> 
  mutate(
    city_state = str_c(city, ", ", state)       # Create city_state variable
  ) |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),  # Total number of homicides
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))  # Number of unsolved homicides
  )

knitr::kable(city_homicides)
```

**For the city of Baltimore, MD**
```{r}
baltimore_df <- 
  city_homicides |>  
  filter(city_state == "Baltimore, MD")

# Use prop.test function to estimate the proportion of homicides that are unsolved and save the output
baltimore_prop_test <- prop.test(
  x = baltimore_df$unsolved_homicides,
  n = baltimore_df$total_homicides
)

# Display the estimated proportion and confidence intervals from the resulting tidy dataframe
tidy_baltimore_test <- broom::tidy(baltimore_prop_test)
tidy_baltimore_test |> 
  select(estimated_proportion = estimate, 
         CI_lower = conf.low,
         CI_upper = conf.high) |> 
  knitr::kable(digits = 4)
```

The estimated proportion of unsolved homicides in Baltimore, MD is `r round(tidy_baltimore_test$estimate, 4)`, with a 95% confidence interval of (`r round(tidy_baltimore_test$conf.low, 4)`, `r round(tidy_baltimore_test$conf.high, 4)`).

**For all cities**
```{r}
# Run prop.test for each of the cities
city_prop_tests <- 
  city_homicides |>  
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, 
                    ~prop.test(x = .x, n = .y)),
    tidy_results = map(prop_test, broom::tidy)
  ) |> 
  unnest(tidy_results) |> 
  select(
    city_state, 
    total_homicides,
    unsolved_homicides,
    proportion_unsolved = estimate, 
    CI_lower = conf.low,
    CI_upper = conf.high
  )

knitr::kable(city_prop_tests, digits = 4)
```

Across all cities, the proportion of unsolved homicides ranges from `r round(min(city_prop_tests$proportion_unsolved), 4)` to `r round(max(city_prop_tests$proportion_unsolved), 4)`, with 95% confidence intervals shown in the table above.

```{r}
# Create a plot that shows the estimates and CIs for each city
city_plot <- 
  city_prop_tests |>
  # Organize cities according to the proportion of unsolved homicides
  mutate(city_state = fct_reorder(city_state, proportion_unsolved)) |> 
  ggplot(aes(x = proportion_unsolved, y = city_state)) +
  # Add points for the estimates
  geom_point(color = "red", size = 1) +
  # Add error bars based on the upper and lower limits
  geom_errorbar(aes(xmin = CI_lower, xmax = CI_upper), 
                width = 0.3, color = "blue", alpha = 0.7) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "Proportion of Unsolved Homicides",
    y = "City"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
        plot.subtitle = element_text(hjust = 0.5))

print(city_plot)
```

This plot shows the proportion of unsolved homicides by city. Each city has a red dot indicating the proportion with error bars, depicting the variation in unsolved homicide proportions across various cities.
